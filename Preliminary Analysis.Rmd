---
fontsize: 10pt
header-includes:
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhf{}
  - \fancyhead[LE,LO]{\leftmark}
  - \fancyhead[RE,RO]{\rightmark}
  - \fancyfoot[CE,CO]{\thepage}
  - \fancyfoot[LE,RO]{}
  - \renewcommand{\headrulewidth}{1 pt}
  - \renewcommand{\footrulewidth}{1 pt}
  - \usepackage{setspace}
  - \setstretch{1.25}
  - \setcounter{page}{1}
  - \usepackage{amsmath}
  - \usepackage{bm}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{amsthm}
  - \usepackage{mathtools}
  - \usepackage{empheq}
  - \usepackage{enumerate}
geometry: "inner = 1 cm, outer = 1 cm, top = 1 cm, bottom = 1 cm, includehead, includefoot"

output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library("latex2exp")
library("ggplot2")
library("MASS")
#library("MCMCpack")
library("coda")
library("dplyr")
library("tidyr")
library("caret")
library("reshape2")
library("DescTools")
```

\markboth{EN.553.671 Final Project}{Jeewon Han and Lucas Rozendaal}
\subsection*{Exploratory Data Analysis}

```{r, warning=FALSE, message=FALSE}
# Exploratory Data Analysis
getwd()
rm(list = ls())

#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad

# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
# Extract the "quality" column
quality_column <- data$quality

# Standardize all columns except the "quality" column
standardized_data <- as.data.frame(scale(data[, -which(names(data) == "quality")]))

# Add the "quality" column back to the standardized dataframe
standardized_data <- cbind(quality = quality_column, standardized_data)

standardized_data$ordinal_quality <- cut(standardized_data$quality, 
                     breaks = c(-Inf, 4.5, 5.5, Inf), 
                     labels = c(1, 2, 3),
                     include.lowest = TRUE)

# If you want to convert the new_column to numeric type
standardized_data$ordinal_quality <- as.numeric(standardized_data$ordinal_quality)

# Summary statistics
summary_stats <- summary(standardized_data)

# Features
features <- c("fixed.acidity", "volatile.acidity", "citric.acid", 
              "residual.sugar", "chlorides", "free.sulfur.dioxide", 
              "total.sulfur.dioxide", "density", "pH", "sulphates", 
              "alcohol")
all_features <- c("fixed.acidity", "volatile.acidity", "citric.acid", 
              "residual.sugar", "chlorides", "free.sulfur.dioxide", 
              "total.sulfur.dioxide", "density", "pH", "sulphates", 
              "alcohol", "quality")

# Pairwise scatterplots
pairwise_scatterplots <- pairs(standardized_data[, all_features])

# Assuming your data frame is named "data"
correlation_matrix <- cor(standardized_data[, all_features])

# Convert the correlation matrix to a long format
correlation_long <- melt(correlation_matrix)

# Create a heatmap using ggplot2
heatmap <- ggplot(data = correlation_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap", x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Histograms for continuous variables
histograms <- lapply(features, function(var) {
  ggplot(standardized_data, aes(x = get(var))) +
    geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = paste("Histogram of", var), x = var, y = "Frequency") +
    theme_minimal()
})

histograms_quality <- ggplot(standardized_data, aes(x = data$quality)) +
    geom_histogram(bins=10, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = "Histogram of Quality", x = "Quality", y = "Frequency") +
    theme_minimal()
  

# Boxplots - Separated out sulfur since scales much larger
# Extracting the variables for separate box plots
sulfur_variables <- c("total.sulfur.dioxide", "free.sulfur.dioxide")
other_variables <- setdiff(all_features, sulfur_variables)

# Boxplot for sulfur variables
boxplot_sulfur <- ggplot(data %>% pivot_longer(cols = sulfur_variables),
                         aes(x = name, y = value)) +
  geom_boxplot(fill = "blue", color = "black", alpha = 0.7, width = 0.5) +
  labs(title = "Boxplots of Sulfur Variables", x = "Variable", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2)))

# Boxplot for other variables
boxplot_other <- ggplot(data %>% pivot_longer(cols = other_variables),
                        aes(x = name, y = value)) +
  geom_boxplot(fill = "blue", color = "black", alpha = 0.7, width = 0.5) +
  labs(title = "Boxplots of Other Variables", x = "Variable", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2)))

# Quantiles, means, medians, and standard deviations
quantiles_means_medians_sds <- standardized_data %>%
  summarise(across(features, list(quantiles = ~quantile(.),
                                      mean = ~mean(.),
                                      median = ~median(.),
                                      sd = ~sd(.),
                                      min = ~min(.),
                                      max = ~max(.))))

# Print summary statistics
print(summary_stats)

# Display pairwise scatterplots
print(pairwise_scatterplots)

# Display Heatmap
print(heatmap)

# Display histograms
for (hist_plot in histograms) {
  print(hist_plot)
}
print(histograms_quality)

# Display boxplot
print(boxplot_sulfur)
print(boxplot_other)

# Display quantiles, means, medians, and standard deviations
print(quantiles_means_medians_sds)

#Plot the probability distribution for quality ratings
# Calculate probabilities of each ranking
rank_probs <- prop.table(table(standardized_data$quality))

# Create a dataframe for probabilities
plot_data <- data.frame(Rank = as.numeric(names(rank_probs)), Probability = as.vector(rank_probs))

# Create the ggplot
ggplot(plot_data, aes(x = Rank, y = Probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black", alpha = 0.7, width = 0.5) +
  labs(title = "Probability Distribution of Quality Ratings",
       x = "Quality Rating",
       y = "Probability") +
  theme_minimal()

```

```{r, warning=FALSE, message=FALSE}
# Function to create a scatterplot of two features based on scatterplot matrix
create_scatterplot <- function(standardized_data, feature1, feature2) {
  ggplot(standardized_data, aes_string(x = feature1, y = feature2)) +
    geom_point(color = "blue", size = 3) +
    labs(title = paste("Scatterplot of", feature1, "vs", feature2),
         x = feature1, y = feature2) +
    theme_minimal()
}

# Example: Create a scatterplot for "fixed.acidity" vs "volatile.acidity"
scatterplot_example <- create_scatterplot(standardized_data, "density", "fixed.acidity")

# Print the scatterplot
print(scatterplot_example)

```

```{r, warning=FALSE, message=FALSE}
# Frequentist Ordinal Regression (on our reduced 3-category ordinal quality column)

# Assuming you have a data frame named "data" containing your features and labels
set.seed(123)  # Set seed for reproducibility
split_index <- createDataPartition(standardized_data$ordinal_quality, p = 0.8, list = FALSE)

# Train/Test Split
train_data <- standardized_data[split_index, ]
test_data <- standardized_data[-split_index, ]

# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
              "residual.sugar", "chlorides", 
              "total.sulfur.dioxide", "density", "sulphates", 
              "alcohol")
  # take out variables with high correlaiton
all_features <- setdiff(features_reg, "ordinal_quality")
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
  # include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value

while (length(selected_features) >= 1) {
  current_accuracy <- 0
  current_num_variables <- length(selected_features)
  worst_feature <- NULL
  exit_loop <- FALSE  # Flag to control loop exit
  
  for (feature in selected_features) {
    current_features <- setdiff(selected_features, feature)
    
    if (length(current_features) == 0) {
      exit_loop <- TRUE
      break  # Exit the inner loop when only one feature is left
    }
    
    # Train the model with the current set of features
    formula_str <- paste("factor(ordinal_quality) ~ ", paste(current_features, collapse = "+"), sep = "")
    model <- polr(as.formula(formula_str), data = train_data, Hess = TRUE)
    
    # Predict using the trained model on the test set
    # (Ensure test_data is also a data frame)
    predicted_labels <- predict(model, newdata = as.data.frame(test_data))
    
    # Convert predicted and true labels to integers (if not already)
    predicted_labels <- as.integer(predicted_labels)
    true_labels <- as.integer(test_data$ordinal_quality)
    
    # Compute accuracy
    accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
    
    # Update the current accuracy, worst feature, and number of variables if needed
    if (accuracy > current_accuracy) {
      current_accuracy <- accuracy
      worst_feature <- feature
      current_num_variables <- length(current_features)
    }
  }
    
  if (exit_loop) {
    break  # Exit the outer loop when only one feature is left
  }
  
  
  # Remove the worst feature from the selected features
  selected_features <- setdiff(selected_features, worst_feature)
  
  # Update the best feature set, accuracy, and number of variables if needed
  if (current_accuracy > best_accuracy || 
      (current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
    best_accuracy <- current_accuracy
    best_feature_set <- selected_features
    best_num_variables <- current_num_variables
  }
  
  cat("Selected features:", selected_features, "\n")
  cat("Current accuracy:", current_accuracy, "\n")
  cat("Current number of variables:", current_num_variables, "\n\n")

}

cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
```

```{r, warning=FALSE, message=FALSE}
# Frequentist Ordinal Regression (on the full 0-10 quality column)

# Assuming you have a data frame named "data" containing your features and labels
set.seed(123)  # Set seed for reproducibility
split_index <- createDataPartition(standardized_data$quality, p = 0.8, list = FALSE)

# Train/Test Split
train_data <- standardized_data[split_index, ]
test_data <- standardized_data[-split_index, ]

# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
              "residual.sugar", "chlorides", 
              "total.sulfur.dioxide", "density", "pH", "sulphates", 
              "alcohol")
  # take out variables with high correlaiton
all_features <- setdiff(features_reg, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
  # include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value

while (length(selected_features) >= 1) {
  current_accuracy <- 0
  current_num_variables <- length(selected_features)
  worst_feature <- NULL
  exit_loop <- FALSE  # Flag to control loop exit
  
  for (feature in selected_features) {
    current_features <- setdiff(selected_features, feature)
    
    if (length(current_features) == 0) {
      exit_loop <- TRUE
      break  # Exit the inner loop when only one feature is left
    }
    
    # Train the model with the current set of features
    formula_str <- paste("factor(quality) ~ ", paste(current_features, collapse = "+"), sep = "")
    model <- polr(as.formula(formula_str), data = train_data, Hess = TRUE)
    
    # Predict using the trained model on the test set
    # (Ensure test_data is also a data frame)
    predicted_labels <- predict(model, newdata = as.data.frame(test_data))
    
    # Convert predicted and true labels to integers (if not already)
    predicted_labels <- as.integer(predicted_labels)
    true_labels <- as.integer(test_data$quality)
    
    # Compute accuracy
    accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
    
    # Update the current accuracy, worst feature, and number of variables if needed
    if (accuracy > current_accuracy) {
      current_accuracy <- accuracy
      worst_feature <- feature
      current_num_variables <- length(current_features)
    }
  }
    
  if (exit_loop) {
    break  # Exit the outer loop when only one feature is left
  }
  
  
  # Remove the worst feature from the selected features
  selected_features <- setdiff(selected_features, worst_feature)
  
  # Update the best feature set, accuracy, and number of variables if needed
  if (current_accuracy > best_accuracy || 
      (current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
    best_accuracy <- current_accuracy
    best_feature_set <- selected_features
    best_num_variables <- current_num_variables
  }
  
  cat("Selected features:", selected_features, "\n")
  cat("Current accuracy:", current_accuracy, "\n")
  cat("Current number of variables:", current_num_variables, "\n\n")

}

cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
```


```{r}
# Baseline model: Assigning the most common quality rating to all observations
most_common_quality <- Mode(train_data$quality)
baseline_prediction <- rep(most_common_quality, nrow(test_data))

# Convert predicted and true labels to integers (if not already)
predicted_labels_baseline <- as.integer(baseline_prediction)
true_labels <- as.integer(test_data$quality)

# Compute accuracy for the baseline model
accuracy_baseline <- sum(predicted_labels_baseline == true_labels) / length(true_labels)

cat("Accuracy for the baseline model:", accuracy_baseline, "\n")

```

```{r, warning=FALSE, message=FALSE}
# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
              "residual.sugar", "chlorides", 
              "total.sulfur.dioxide", "density", "pH", "sulphates", 
              "alcohol")
  # take out variables with high correlation
all_features <- setdiff(features, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
  # include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value

while (length(selected_features) >= 1) {
  current_accuracy <- 0
  current_num_variables <- length(selected_features)
  worst_feature <- NULL
  exit_loop <- FALSE  # Flag to control loop exit
  
  for (feature in selected_features) {
    current_features <- setdiff(selected_features, feature)
    
    if (length(current_features) == 0) {
      exit_loop <- TRUE
      break  # Exit the inner loop when only one feature is left
    }
    
    # Train the model with the current set of features
    formula_str <- paste("quality ~ ", paste(current_features, collapse = "+"), sep = "")
    model <- lm(as.formula(formula_str), data = train_data, Hess = TRUE)
    
    # Predict using the trained model on the test set
    # (Ensure test_data is also a data frame)
    predicted_labels <- predict(model, newdata = as.data.frame(test_data))
    
    # Convert predicted and true labels to integers (if not already)
    predicted_labels <- as.integer(predicted_labels)
    true_labels <- as.integer(test_data$quality)
    
    # Compute accuracy
    accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
    
    # Update the current accuracy, worst feature, and number of variables if needed
    if (accuracy > current_accuracy) {
      current_accuracy <- accuracy
      worst_feature <- feature
        # worst feature = feature that results in highest increase in accuracy when removed
      current_num_variables <- length(current_features)
    }
  }
    
  if (exit_loop) {
    break  # Exit the outer loop when only one feature is left
  }
  
  
  # Remove the worst feature from the selected features
  selected_features <- setdiff(selected_features, worst_feature)
  
  # Update the best feature set, accuracy, and number of variables if needed
  if (current_accuracy > best_accuracy || 
      (current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
    best_accuracy <- current_accuracy
    best_feature_set <- selected_features
    best_num_variables <- current_num_variables
  }
  
  cat("Selected features:", selected_features, "\n")
  cat("Current accuracy:", current_accuracy, "\n")
  cat("Current number of variables:", current_num_variables, "\n\n")

}

cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
```

```{r}
# Baseline model: Assigning the most common quality rating to all observations
most_common_quality <- Mode(train_data$ordinal_quality)
baseline_prediction <- rep(most_common_quality, nrow(test_data))

# Convert predicted and true labels to integers (if not already)
predicted_labels_baseline <- as.integer(baseline_prediction)
true_labels <- as.integer(test_data$ordinal_quality)

# Compute accuracy for the baseline model
accuracy_baseline <- sum(predicted_labels_baseline == true_labels) / length(true_labels)

cat("Accuracy for the baseline model:", accuracy_baseline, "\n")

```