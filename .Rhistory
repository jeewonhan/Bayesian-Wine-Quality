set.seed(123)  # Set seed for reproducibility
split_index <- createDataPartition(data$quality, p = 0.8, list = FALSE)
# Train/Test Split
train_data <- data[split_index, ]
test_data <- data[-split_index, ]
# Backward elimination for the most parsimonious model
all_features <- setdiff(names(train_data), "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
# include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value
while (length(selected_features) >= 1) {
current_accuracy <- 0
current_num_variables <- length(selected_features)
worst_feature <- NULL
exit_loop <- FALSE  # Flag to control loop exit
for (feature in selected_features) {
current_features <- setdiff(selected_features, feature)
if (length(current_features) == 0) {
exit_loop <- TRUE
break  # Exit the inner loop when only one feature is left
}
# Train the model with the current set of features
formula_str <- paste("factor(quality) ~ ", paste(current_features, collapse = "+"), sep = "")
model <- polr(as.formula(formula_str), data = train_data, Hess = TRUE)
# Predict using the trained model on the test set
# (Ensure test_data is also a data frame)
predicted_labels <- predict(model, newdata = as.data.frame(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels <- as.integer(predicted_labels)
true_labels <- as.integer(test_data$quality)
# Compute accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
# Update the current accuracy, worst feature, and number of variables if needed
if (accuracy > current_accuracy) {
current_accuracy <- accuracy
worst_feature <- feature
current_num_variables <- length(current_features)
}
}
if (exit_loop) {
break  # Exit the outer loop when only one feature is left
}
# Remove the worst feature from the selected features
selected_features <- setdiff(selected_features, worst_feature)
# Update the best feature set, accuracy, and number of variables if needed
if (current_accuracy > best_accuracy ||
(current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
best_accuracy <- current_accuracy
best_feature_set <- selected_features
best_num_variables <- current_num_variables
}
cat("Selected features:", selected_features, "\n")
cat("Current accuracy:", current_accuracy, "\n")
cat("Current number of variables:", current_num_variables, "\n\n")
}
cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
# Baseline model: Assigning the most common quality rating to all observations
most_common_quality <- mode(data$quality)[1]
baseline_prediction <- rep(most_common_quality, nrow(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels_baseline <- as.integer(baseline_prediction)
true_labels <- as.integer(test_data$quality)
# Compute accuracy for the baseline model
accuracy_baseline <- sum(predicted_labels_baseline == true_labels) / length(true_labels)
cat("Accuracy for the baseline model:", accuracy_baseline, "\n")
print(mode(data$quality))
# Baseline model: Assigning the most common quality rating to all observations
most_common_quality <- mode(data$quality)[1]
baseline_prediction <- rep(most_common_quality, nrow(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels_baseline <- as.integer(baseline_prediction)
true_labels <- as.integer(test_data$quality)
# Compute accuracy for the baseline model
accuracy_baseline <- sum(predicted_labels_baseline == true_labels) / length(true_labels)
cat("Accuracy for the baseline model:", accuracy_baseline, "\n")
View(data)
library("latex2exp")
library("ggplot2")
library("MASS")
#library("MCMCpack")
library("coda")
library("dplyr")
library("tidyr")
library("caret")
library("reshape2")
library("DescTools")
# Baseline model: Assigning the most common quality rating to all observations
most_common_quality <- Mode(train_data$quality)
baseline_prediction <- rep(most_common_quality, nrow(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels_baseline <- as.integer(baseline_prediction)
true_labels <- as.integer(test_data$quality)
# Compute accuracy for the baseline model
accuracy_baseline <- sum(predicted_labels_baseline == true_labels) / length(true_labels)
cat("Accuracy for the baseline model:", accuracy_baseline, "\n")
# Frequentist Ordinal Regression
# Assuming you have a data frame named "data" containing your features and labels
set.seed(123)  # Set seed for reproducibility
split_index <- createDataPartition(data$quality, p = 0.8, list = FALSE)
# Train/Test Split
train_data <- data[split_index, ]
test_data <- data[-split_index, ]
# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
"residual.sugar", "chlorides",
"total.sulfur.dioxide", "density", "pH", "sulphates",
"alcohol")
all_features <- setdiff(features_reg, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
# include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value
while (length(selected_features) >= 1) {
current_accuracy <- 0
current_num_variables <- length(selected_features)
worst_feature <- NULL
exit_loop <- FALSE  # Flag to control loop exit
for (feature in selected_features) {
current_features <- setdiff(selected_features, feature)
if (length(current_features) == 0) {
exit_loop <- TRUE
break  # Exit the inner loop when only one feature is left
}
# Train the model with the current set of features
formula_str <- paste("factor(quality) ~ ", paste(current_features, collapse = "+"), sep = "")
model <- polr(as.formula(formula_str), data = train_data, Hess = TRUE)
# Predict using the trained model on the test set
# (Ensure test_data is also a data frame)
predicted_labels <- predict(model, newdata = as.data.frame(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels <- as.integer(predicted_labels)
true_labels <- as.integer(test_data$quality)
# Compute accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
# Update the current accuracy, worst feature, and number of variables if needed
if (accuracy > current_accuracy) {
current_accuracy <- accuracy
worst_feature <- feature
current_num_variables <- length(current_features)
}
}
if (exit_loop) {
break  # Exit the outer loop when only one feature is left
}
# Remove the worst feature from the selected features
selected_features <- setdiff(selected_features, worst_feature)
# Update the best feature set, accuracy, and number of variables if needed
if (current_accuracy > best_accuracy ||
(current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
best_accuracy <- current_accuracy
best_feature_set <- selected_features
best_num_variables <- current_num_variables
}
cat("Selected features:", selected_features, "\n")
cat("Current accuracy:", current_accuracy, "\n")
cat("Current number of variables:", current_num_variables, "\n\n")
}
cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
# Frequentist Ordinal Regression
# Assuming you have a data frame named "data" containing your features and labels
set.seed(123)  # Set seed for reproducibility
split_index <- createDataPartition(data$quality, p = 0.8, list = FALSE)
# Train/Test Split
train_data <- data[split_index, ]
test_data <- data[-split_index, ]
# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
"residual.sugar", "chlorides",
"total.sulfur.dioxide", "density", "pH", "sulphates",
"alcohol")
# take out variables with high correlaiton
all_features <- setdiff(features_reg, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
# include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value
while (length(selected_features) >= 1) {
current_accuracy <- 0
current_num_variables <- length(selected_features)
worst_feature <- NULL
exit_loop <- FALSE  # Flag to control loop exit
for (feature in selected_features) {
current_features <- setdiff(selected_features, feature)
if (length(current_features) == 0) {
exit_loop <- TRUE
break  # Exit the inner loop when only one feature is left
}
# Train the model with the current set of features
formula_str <- paste("factor(quality) ~ ", paste(current_features, collapse = "+"), sep = "")
model <- polr(as.formula(formula_str), data = train_data, Hess = TRUE)
# Predict using the trained model on the test set
# (Ensure test_data is also a data frame)
predicted_labels <- predict(model, newdata = as.data.frame(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels <- as.integer(predicted_labels)
true_labels <- as.integer(test_data$quality)
# Compute accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
# Update the current accuracy, worst feature, and number of variables if needed
if (accuracy > current_accuracy) {
current_accuracy <- accuracy
worst_feature <- feature
current_num_variables <- length(current_features)
}
}
if (exit_loop) {
break  # Exit the outer loop when only one feature is left
}
# Remove the worst feature from the selected features
selected_features <- setdiff(selected_features, worst_feature)
# Update the best feature set, accuracy, and number of variables if needed
if (current_accuracy > best_accuracy ||
(current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
best_accuracy <- current_accuracy
best_feature_set <- selected_features
best_num_variables <- current_num_variables
}
cat("Selected features:", selected_features, "\n")
cat("Current accuracy:", current_accuracy, "\n")
cat("Current number of variables:", current_num_variables, "\n\n")
# Calculate McFadden's pseudo-R^2
pseudo_r2_mcfadden <- 1 - (logLik(model) / logLik(nullmodel(model)))
# Calculate Tjur's pseudo-R^2
pseudo_r2_tjur <- sum(predict(model, type = "probs")^2) / length(model$y) - 1 / length(model$y)
# Print pseudo-R^2 values
cat("McFadden's pseudo-R^2:", pseudo_r2_mcfadden, "\n")
cat("Tjur's pseudo-R^2:", pseudo_r2_tjur, "\n")
}
# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
"residual.sugar", "chlorides",
"total.sulfur.dioxide", "density", "pH", "sulphates",
"alcohol")
# take out variables with high correlaiton
all_features <- setdiff(features_reg, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
# include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value
while (length(selected_features) >= 1) {
current_accuracy <- 0
current_num_variables <- length(selected_features)
worst_feature <- NULL
exit_loop <- FALSE  # Flag to control loop exit
for (feature in selected_features) {
current_features <- setdiff(selected_features, feature)
if (length(current_features) == 0) {
exit_loop <- TRUE
break  # Exit the inner loop when only one feature is left
}
# Train the model with the current set of features
formula_str <- paste("quality ~ ", paste(current_features, collapse = "+"), sep = "")
model <- lm(as.formula(formula_str), data = train_data, Hess = TRUE)
# Predict using the trained model on the test set
# (Ensure test_data is also a data frame)
predicted_labels <- predict(model, newdata = as.data.frame(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels <- as.integer(predicted_labels)
true_labels <- as.integer(test_data$quality)
# Compute accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
# Update the current accuracy, worst feature, and number of variables if needed
if (accuracy > current_accuracy) {
current_accuracy <- accuracy
worst_feature <- feature
current_num_variables <- length(current_features)
}
}
if (exit_loop) {
break  # Exit the outer loop when only one feature is left
}
# Remove the worst feature from the selected features
selected_features <- setdiff(selected_features, worst_feature)
# Update the best feature set, accuracy, and number of variables if needed
if (current_accuracy > best_accuracy ||
(current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
best_accuracy <- current_accuracy
best_feature_set <- selected_features
best_num_variables <- current_num_variables
}
cat("Selected features:", selected_features, "\n")
cat("Current accuracy:", current_accuracy, "\n")
cat("Current number of variables:", current_num_variables, "\n\n")
}
cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
"residual.sugar", "chlorides",
"total.sulfur.dioxide", "density", "pH", "sulphates",
"alcohol")
# take out variables with high correlaiton
all_features <- setdiff(features_reg, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
# include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value
while (length(selected_features) >= 1) {
current_accuracy <- 0
current_num_variables <- length(selected_features)
worst_feature <- NULL
exit_loop <- FALSE  # Flag to control loop exit
for (feature in selected_features) {
current_features <- setdiff(selected_features, feature)
if (length(current_features) == 0) {
exit_loop <- TRUE
break  # Exit the inner loop when only one feature is left
}
# Train the model with the current set of features
formula_str <- paste("quality ~ ", paste(current_features, collapse = "+"), sep = "")
model <- lm(as.formula(formula_str), data = train_data, Hess = TRUE)
# Predict using the trained model on the test set
# (Ensure test_data is also a data frame)
predicted_labels <- predict(model, newdata = as.data.frame(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels <- as.integer(predicted_labels)
true_labels <- as.integer(test_data$quality)
# Compute accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
# Update the current accuracy, worst feature, and number of variables if needed
if (accuracy > current_accuracy) {
current_accuracy <- accuracy
worst_feature <- feature
current_num_variables <- length(current_features)
}
}
if (exit_loop) {
break  # Exit the outer loop when only one feature is left
}
# Remove the worst feature from the selected features
selected_features <- setdiff(selected_features, worst_feature)
# Update the best feature set, accuracy, and number of variables if needed
if (current_accuracy > best_accuracy ||
(current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
best_accuracy <- current_accuracy
best_feature_set <- selected_features
best_num_variables <- current_num_variables
}
cat("Selected features:", selected_features, "\n")
cat("Current accuracy:", current_accuracy, "\n")
cat("Current number of variables:", current_num_variables, "\n\n")
}
cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
# Backward elimination for the most parsimonious model
features_reg <- c("fixed.acidity",
"residual.sugar", "chlorides",
"total.sulfur.dioxide", "density", "pH", "sulphates",
"alcohol")
# take out variables with high correlaiton
all_features <- setdiff(features, "quality")
#selected_features <- all_features
selected_features <- c(all_features, "fixed.acidity*density", "alcohol*density")
# include interaction terms
best_accuracy <- 0
best_feature_set <- NULL
best_num_variables <- Inf  # Initialize with a large value
while (length(selected_features) >= 1) {
current_accuracy <- 0
current_num_variables <- length(selected_features)
worst_feature <- NULL
exit_loop <- FALSE  # Flag to control loop exit
for (feature in selected_features) {
current_features <- setdiff(selected_features, feature)
if (length(current_features) == 0) {
exit_loop <- TRUE
break  # Exit the inner loop when only one feature is left
}
# Train the model with the current set of features
formula_str <- paste("quality ~ ", paste(current_features, collapse = "+"), sep = "")
model <- lm(as.formula(formula_str), data = train_data, Hess = TRUE)
# Predict using the trained model on the test set
# (Ensure test_data is also a data frame)
predicted_labels <- predict(model, newdata = as.data.frame(test_data))
# Convert predicted and true labels to integers (if not already)
predicted_labels <- as.integer(predicted_labels)
true_labels <- as.integer(test_data$quality)
# Compute accuracy
accuracy <- sum(predicted_labels == true_labels) / length(true_labels)
# Update the current accuracy, worst feature, and number of variables if needed
if (accuracy > current_accuracy) {
current_accuracy <- accuracy
worst_feature <- feature
current_num_variables <- length(current_features)
}
}
if (exit_loop) {
break  # Exit the outer loop when only one feature is left
}
# Remove the worst feature from the selected features
selected_features <- setdiff(selected_features, worst_feature)
# Update the best feature set, accuracy, and number of variables if needed
if (current_accuracy > best_accuracy ||
(current_accuracy == best_accuracy && current_num_variables < best_num_variables)) {
best_accuracy <- current_accuracy
best_feature_set <- selected_features
best_num_variables <- current_num_variables
}
cat("Selected features:", selected_features, "\n")
cat("Current accuracy:", current_accuracy, "\n")
cat("Current number of variables:", current_num_variables, "\n\n")
}
cat("Best feature set:", best_feature_set, "\n")
cat("Best accuracy:", best_accuracy, "\n")
cat("Best number of variables:", best_num_variables, "\n")
knitr::opts_chunk$set(echo = TRUE)
library("latex2exp")
library("ggplot2")
library("MASS")
#library("MCMCpack")
library("coda")
library("dplyr")
library("tidyr")
library("caret")
library("reshape2")
library("DescTools")
# Exploratory Data Analysis
getwd()
rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
# Exploratory Data Analysis
#getwd()
#rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
library("latex2exp")
library("ggplot2")
library("MASS")
#library("MCMCpack")
library("coda")
library("dplyr")
library("tidyr")
library("caret")
library("reshape2")
library("DescTools")
# Exploratory Data Analysis
#getwd()
#rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
setwd("~/Documents/College/Master's Year/Fall Semester/Bayesian Stats/Final Project/GitHub Repository")
# Exploratory Data Analysis
getwd()
rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
setwd("~/Documents/College/Master's Year/Fall Semester/Bayesian Stats/Final Project/GitHub Repository")
# Exploratory Data Analysis
getwd()
rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
knitr::opts_chunk$set(echo = TRUE)
library("latex2exp")
library("ggplot2")
library("MASS")
#library("MCMCpack")
library("coda")
library("dplyr")
library("tidyr")
library("caret")
library("reshape2")
library("DescTools")
# Exploratory Data Analysis
getwd()
rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';')
# Exploratory Data Analysis
getwd()
rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';' header=TRUE)
# Exploratory Data Analysis
getwd()
rm(list = ls())
#try standardizing the data and seeing if that helps
#try splitting the categories into good vs bad
# Read your CSV data
data <- read.csv("winequality-red.csv", sep=';', header=TRUE)
